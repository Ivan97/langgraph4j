{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain4j LLM streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "String userHomeDir = System.getProperty(\"user.home\");\n",
    "String localRespoUrl = \"file://\" + userHomeDir + \"/.m2/repository/\";\n",
    "String langchain4jVersion = \"0.36.2\";\n",
    "String langgraph4jVersion = \"1.2-SNAPSHOT\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mRepository \u001b[1m\u001b[32mlocal\u001b[0m url: \u001b[1m\u001b[32mfile:///Users/bsorrentino/.m2/repository/\u001b[0m added.\n",
      "\u001b[0mRepositories count: 4\n",
      "\u001b[0mname: \u001b[1m\u001b[32mcentral \u001b[0murl: \u001b[1m\u001b[32mhttps://repo.maven.apache.org/maven2/ \u001b[0mrelease:\u001b[32mtrue \u001b[0mupdate:\u001b[32mnever \u001b[0msnapshot:\u001b[32mfalse \u001b[0mupdate:\u001b[32mnever \n",
      "\u001b[0m\u001b[0mname: \u001b[1m\u001b[32mjboss \u001b[0murl: \u001b[1m\u001b[32mhttps://repository.jboss.org/nexus/content/repositories/releases/ \u001b[0mrelease:\u001b[32mtrue \u001b[0mupdate:\u001b[32mnever \u001b[0msnapshot:\u001b[32mfalse \u001b[0mupdate:\u001b[32mnever \n",
      "\u001b[0m\u001b[0mname: \u001b[1m\u001b[32matlassian \u001b[0murl: \u001b[1m\u001b[32mhttps://packages.atlassian.com/maven/public \u001b[0mrelease:\u001b[32mtrue \u001b[0mupdate:\u001b[32mnever \u001b[0msnapshot:\u001b[32mfalse \u001b[0mupdate:\u001b[32mnever \n",
      "\u001b[0m\u001b[0mname: \u001b[1m\u001b[32mlocal \u001b[0murl: \u001b[1m\u001b[32mfile:///Users/bsorrentino/.m2/repository/ \u001b[0mrelease:\u001b[32mtrue \u001b[0mupdate:\u001b[32mnever \u001b[0msnapshot:\u001b[32mtrue \u001b[0mupdate:\u001b[32malways \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%dependency /add-repo local \\{localRespoUrl} release|never snapshot|always\n",
    "%dependency /list-repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove installed package from Jupiter cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "rm -rf \\{userHomeDir}/Library/Jupyter/kernels/rapaio-jupyter-kernel/mima_cache/org/bsc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dependency /add org.slf4j:slf4j-jdk14:2.0.9\n",
    "%dependency /add org.bsc.langgraph4j:langgraph4j-core:\\{langgraph4jVersion}\n",
    "%dependency /add org.bsc.langgraph4j:langgraph4j-langchain4j:\\{langgraph4jVersion}\n",
    "%dependency /add dev.langchain4j:langchain4j:\\{langchain4jVersion}\n",
    "%dependency /add dev.langchain4j:langchain4j-open-ai:\\{langchain4jVersion}\n",
    "\n",
    "%dependency /resolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Logger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try( var file = new java.io.FileInputStream(\"./logging.properties\")) {\n",
    "    var lm = java.util.logging.LogManager.getLogManager();\n",
    "    lm.checkAccess(); \n",
    "    lm.readConfiguration( file );\n",
    "}\n",
    "\n",
    "var log = org.slf4j.LoggerFactory.getLogger(\"llm-streaming\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use LLMStreamingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dev.langchain4j.model.StreamingResponseHandler;\n",
    "import dev.langchain4j.model.chat.StreamingChatLanguageModel;\n",
    "import dev.langchain4j.model.openai.OpenAiStreamingChatModel;\n",
    "import dev.langchain4j.data.message.AiMessage;\n",
    "import dev.langchain4j.model.output.Response;\n",
    "import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;\n",
    "import org.bsc.langgraph4j.langchain4j.generators.LLMStreamingGenerator;\n",
    "import org.bsc.langgraph4j.state.AgentState;\n",
    "import org.bsc.langgraph4j.streaming.StreamingOutput;\n",
    "\n",
    "var generator = LLMStreamingGenerator.<AiMessage,AgentState>builder()\n",
    "                        .mapResult( r -> Map.of( \"content\", r.content() ) )\n",
    "                        .build();\n",
    "\n",
    "StreamingChatLanguageModel model = OpenAiStreamingChatModel.builder()\n",
    "    .apiKey(System.getenv(\"OPENAI_API_KEY\"))\n",
    "    .modelName(GPT_4_O_MINI)\n",
    "    .build();\n",
    "\n",
    "String userMessage = \"Tell me a joke\";\n",
    "\n",
    "model.generate(userMessage, generator.handler() );\n",
    "\n",
    "for( var r : generator ) {\n",
    "    log.info( \"{}\", r);\n",
    "}\n",
    "  \n",
    "log.info( \"RESULT: {}\", generator.resultValue().orElse(null) );\n",
    "  \n",
    "//Thread.sleep( 1000 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLMStreamGenerator in Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.bsc.langgraph4j.state.AgentState;\n",
    "import org.bsc.langgraph4j.state.Channel;\n",
    "import org.bsc.langgraph4j.state.AppenderChannel;\n",
    "import dev.langchain4j.data.message.ChatMessage;\n",
    "import dev.langchain4j.data.message.UserMessage;\n",
    "\n",
    "public class MessageState extends AgentState {\n",
    "\n",
    "    static Map<String, Channel<?>> SCHEMA = Map.of(\n",
    "            \"messages\", AppenderChannel.<ChatMessage>of(ArrayList::new)\n",
    "    );\n",
    "\n",
    "    public MessageState(Map<String, Object> initData) {\n",
    "        super( initData  );\n",
    "    }\n",
    "\n",
    "    List<ChatMessage> messages() {\n",
    "        return this.<List<ChatMessage>>value( \"messages\" )\n",
    "                .orElseThrow( () -> new RuntimeException( \"messages not found\" ) );\n",
    "    }\n",
    "\n",
    "    // utility method to quick access to last message\n",
    "    Optional<ChatMessage> lastMessage() {\n",
    "        List<ChatMessage> messages = messages();\n",
    "        return ( messages.isEmpty() ) ? \n",
    "            Optional.empty() :\n",
    "            Optional.of(messages.get( messages.size() - 1 ));\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dev.langchain4j.data.message.AiMessage;\n",
    "import dev.langchain4j.data.message.SystemMessage;\n",
    "import dev.langchain4j.data.message.UserMessage;\n",
    "import dev.langchain4j.data.message.ToolExecutionResultMessage;\n",
    "import dev.langchain4j.agent.tool.ToolExecutionRequest;\n",
    "import org.bsc.langgraph4j.serializer.std.ObjectStreamStateSerializer;\n",
    "import org.bsc.langgraph4j.langchain4j.serializer.std.ChatMesssageSerializer;\n",
    "import org.bsc.langgraph4j.langchain4j.serializer.std.ToolExecutionRequestSerializer;\n",
    "import org.bsc.langgraph4j.state.AgentStateFactory;\n",
    "\n",
    "var stateSerializer = new ObjectStreamStateSerializer<MessageState>( MessageState::new );\n",
    "stateSerializer.mapper()\n",
    "    // Setup custom serializer for Langchain4j ToolExecutionRequest\n",
    "    .register(ToolExecutionRequest.class, new ToolExecutionRequestSerializer() )\n",
    "    // Setup custom serializer for Langchain4j AiMessage\n",
    "    .register(ChatMessage.class, new ChatMesssageSerializer() );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "Using [langchain4j], We will first define the tools we want to use. For this simple example, we will\n",
    "use create a placeholder search engine. However, it is really easy to create\n",
    "your own tools - see documentation\n",
    "[here][tools] on how to do\n",
    "that.\n",
    "\n",
    "[langchain4j]: https://docs.langchain4j.dev\n",
    "[tools]: https://docs.langchain4j.dev/tutorials/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dev.langchain4j.agent.tool.P;\n",
    "import dev.langchain4j.agent.tool.Tool;\n",
    "\n",
    "import java.util.Optional;\n",
    "\n",
    "import static java.lang.String.format;\n",
    "\n",
    "public class SearchTool {\n",
    "\n",
    "    @Tool(\"Use to surf the web, fetch current information, check the weather, and retrieve other information.\")\n",
    "    String execQuery(@P(\"The query to use in your search.\") String query) {\n",
    "\n",
    "        // This is a placeholder for the actual implementation\n",
    "        return \"Cold, with a low of 13 degrees\";\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import static org.bsc.langgraph4j.StateGraph.START;\n",
    "import static org.bsc.langgraph4j.StateGraph.END;\n",
    "import org.bsc.langgraph4j.StateGraph;\n",
    "import org.bsc.langgraph4j.action.EdgeAction;\n",
    "import static org.bsc.langgraph4j.action.AsyncEdgeAction.edge_async;\n",
    "import org.bsc.langgraph4j.action.NodeAction;\n",
    "import static org.bsc.langgraph4j.action.AsyncNodeAction.node_async;\n",
    "import dev.langchain4j.service.tool.DefaultToolExecutor;\n",
    "import org.bsc.langgraph4j.langchain4j.tool.ToolNode;\n",
    "import dev.langchain4j.agent.tool.ToolSpecification;\n",
    "import dev.langchain4j.model.openai.OpenAiStreamingChatModel;\n",
    "\n",
    "\n",
    "// setup streaming model\n",
    "var model = OpenAiStreamingChatModel.builder()\n",
    "  .apiKey( System.getenv(\"OPENAI_API_KEY\") )\n",
    "  .modelName( \"gpt-4o-mini\" )\n",
    "  .logResponses(true)\n",
    "  .temperature(0.0)\n",
    "  .maxTokens(2000)\n",
    "  .build();\n",
    "\n",
    "// setup tools \n",
    "var tools = ToolNode.builder()\n",
    "              .specification( new SearchTool() ) \n",
    "              .build(); \n",
    "\n",
    "NodeAction<MessageState> callModel = state -> {\n",
    "  log.info(\"CallModel:\\n{}\", state.messages());\n",
    "\n",
    "  var generator = LLMStreamingGenerator.<AiMessage, MessageState>builder()\n",
    "          .mapResult(response -> {\n",
    "              log.info(\"MapResult: {}\", response);\n",
    "              return Map.of(\"messages\", response.content());\n",
    "          })\n",
    "          .startingNode(\"agent\")\n",
    "          .startingState(state)\n",
    "          .build();\n",
    "\n",
    "    model.generate(\n",
    "            state.messages(),\n",
    "            tools.toolSpecifications(),\n",
    "            generator.handler());\n",
    "\n",
    "    return Map.of(\"messages\", generator);\n",
    "};\n",
    "            \n",
    "// Route Message\n",
    "EdgeAction<MessageState> routeMessage = state -> {\n",
    "    log.info(\"routeMessage:\\n{}\", state.messages());\n",
    "\n",
    "    var lastMessage = state.lastMessage()\n",
    "            .orElseThrow(() -> (new IllegalStateException(\"last message not found!\")));\n",
    "\n",
    "    if (lastMessage instanceof AiMessage message) {\n",
    "        // If tools should be called\n",
    "        if (message.hasToolExecutionRequests()) return \"next\";\n",
    "    }\n",
    "\n",
    "    // If no tools are called, we can finish (respond to the user)\n",
    "    return \"exit\";\n",
    "};\n",
    "            \n",
    "// Invoke Tool\n",
    "NodeAction<MessageState> invokeTool = state -> {\n",
    "    log.info(\"invokeTool:\\n{}\", state.messages());\n",
    "\n",
    "    var lastMessage = state.lastMessage()\n",
    "            .orElseThrow(() -> (new IllegalStateException(\"last message not found!\")));\n",
    "\n",
    "\n",
    "    if (lastMessage instanceof AiMessage lastAiMessage) {\n",
    "\n",
    "        var result = tools.execute(lastAiMessage.toolExecutionRequests(), null)\n",
    "                .orElseThrow(() -> (new IllegalStateException(\"no tool found!\")));\n",
    "\n",
    "        return Map.of(\"messages\", result);\n",
    "\n",
    "    }\n",
    "\n",
    "    throw new IllegalStateException(\"invalid last message\");\n",
    "};\n",
    "            \n",
    "// Define Graph\n",
    "var workflow = new StateGraph<MessageState>(MessageState.SCHEMA, stateSerializer)\n",
    "        .addNode(\"agent\", node_async(callModel))\n",
    "        .addNode(\"tools\", node_async(invokeTool))\n",
    "        .addEdge(START, \"agent\")\n",
    "        .addConditionalEdges(\"agent\",\n",
    "                edge_async(routeMessage),\n",
    "                Map.of(\"next\", \"tools\", \"exit\", END))\n",
    "        .addEdge(\"tools\", \"agent\");\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "START \n",
      "CallModel:\n",
      "[UserMessage { name = null contents = [TextContent { text = \"what is the whether today?\" }] }] \n",
      "MapResult: Response { content = AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }, tokenUsage = TokenUsage { inputTokenCount = 71, outputTokenCount = 15, totalTokenCount = 86 }, finishReason = TOOL_EXECUTION, metadata = {} } \n",
      "routeMessage:\n",
      "[AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }] \n",
      "NodeOutput{node=__START__, state={messages=[UserMessage { name = null contents = [TextContent { text = \"what is the whether today?\" }] }]}} \n",
      "invokeTool:\n",
      "[AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }] \n",
      "execute: execQuery \n",
      "NodeOutput{node=agent, state={messages=[AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }]}} \n",
      "CallModel:\n",
      "[AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }, ToolExecutionResultMessage { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\" toolName = \"execQuery\" text = \"Cold, with a low of 13 degrees\" }] \n",
      "NodeOutput{node=tools, state={messages=[AiMessage { text = null toolExecutionRequests = [ToolExecutionRequest { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\", name = \"execQuery\", arguments = \"{\"query\":\"current weather\"}\" }] }, ToolExecutionResultMessage { id = \"call_ycvEnAG6NCrreo0WqSoxS5jJ\" toolName = \"execQuery\" text = \"Cold, with a low of 13 degrees\" }]}} \n",
      "StreamingOutput{node=agent, chunk= } \n",
      "StreamingOutput{node=agent, chunk=The } \n",
      "StreamingOutput{node=agent, chunk= current } \n",
      "StreamingOutput{node=agent, chunk= weather } \n",
      "StreamingOutput{node=agent, chunk= is } \n",
      "StreamingOutput{node=agent, chunk= cold } \n",
      "StreamingOutput{node=agent, chunk=, } \n",
      "StreamingOutput{node=agent, chunk= with } \n",
      "StreamingOutput{node=agent, chunk= a } \n",
      "StreamingOutput{node=agent, chunk= low } \n",
      "StreamingOutput{node=agent, chunk= of } \n",
      "StreamingOutput{node=agent, chunk=  } \n",
      "StreamingOutput{node=agent, chunk=13 } \n",
      "StreamingOutput{node=agent, chunk= degrees } \n",
      "StreamingOutput{node=agent, chunk=. } \n",
      "StreamingOutput{node=agent, chunk= If } \n",
      "StreamingOutput{node=agent, chunk= you } \n",
      "StreamingOutput{node=agent, chunk= need } \n",
      "StreamingOutput{node=agent, chunk= more } \n",
      "StreamingOutput{node=agent, chunk= specific } \n",
      "StreamingOutput{node=agent, chunk= information } \n",
      "StreamingOutput{node=agent, chunk= or } \n",
      "StreamingOutput{node=agent, chunk= details } \n",
      "StreamingOutput{node=agent, chunk= about } \n",
      "StreamingOutput{node=agent, chunk= a } \n",
      "StreamingOutput{node=agent, chunk= particular } \n",
      "StreamingOutput{node=agent, chunk= location } \n",
      "StreamingOutput{node=agent, chunk=, } \n",
      "StreamingOutput{node=agent, chunk= feel } \n",
      "StreamingOutput{node=agent, chunk= free } \n",
      "StreamingOutput{node=agent, chunk= to } \n",
      "StreamingOutput{node=agent, chunk= ask } \n",
      "MapResult: Response { content = AiMessage { text = \"The current weather is cold, with a low of 13 degrees. If you need more specific information or details about a particular location, feel free to ask!\" toolExecutionRequests = null }, tokenUsage = TokenUsage { inputTokenCount = 93, outputTokenCount = 33, totalTokenCount = 126 }, finishReason = STOP, metadata = {} } \n",
      "routeMessage:\n",
      "[AiMessage { text = \"The current weather is cold, with a low of 13 degrees. If you need more specific information or details about a particular location, feel free to ask!\" toolExecutionRequests = null }] \n",
      "StreamingOutput{node=agent, chunk=! } \n",
      "NodeOutput{node=agent, state={messages=[AiMessage { text = \"The current weather is cold, with a low of 13 degrees. If you need more specific information or details about a particular location, feel free to ask!\" toolExecutionRequests = null }]}} \n",
      "NodeOutput{node=__END__, state={messages=[AiMessage { text = \"The current weather is cold, with a low of 13 degrees. If you need more specific information or details about a particular location, feel free to ask!\" toolExecutionRequests = null }]}} \n"
     ]
    }
   ],
   "source": [
    "import org.bsc.langgraph4j.streaming.StreamingOutput;\n",
    "\n",
    "var app = workflow.compile();\n",
    "\n",
    "for( var out : app.stream( Map.of( \"messages\", UserMessage.from( \"what is the whether today?\")) ) ) {\n",
    "  if( out instanceof StreamingOutput streaming ) {\n",
    "    log.info( \"StreamingOutput{node={}, chunk={} }\", streaming.node(), streaming.chunk() );\n",
    "  }\n",
    "  else {\n",
    "    log.info( \"{}\", out );\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java (rjk 2.2.0)",
   "language": "java",
   "name": "rapaio-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "java",
   "nbconvert_exporter": "script",
   "pygments_lexer": "java",
   "version": "17.0.2+8-86"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
